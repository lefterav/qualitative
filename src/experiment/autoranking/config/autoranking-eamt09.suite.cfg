[DEFAULT]
repetitions = 1
iterations = 130
path = /share/taraxu/selection-mechanism/eamt09/autoranking

experiment = grid
mode = "production"

langpair = ["en-es"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/selection-mechanism/eamt09/annotation/eamt09-train-en-es.all.analyzed.f.jcml"
test_set = "/share/taraxu/selection-mechanism/eamt09/annotation/eamt09-test-en-es.all.analyzed.f.jcml"

class_name = ["rank"]
class_type = "d"

classifier = ["LibLinearLogReg", "LogReg", "SVMEasy", "kNN", "Naive"]


params_LogReg = "{'stepwise_lr':True, 'remove_singular':True}"

attset_1_source = 
attset_1_target = 
attset_1_general = 

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = 

#succesful features from our WMT11 metric with no ratios
attset_21_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_target = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_general = 

#succesful features from our WMT11 metric with no ratios + pseudoMETEOR
attset_24_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_24_target = "cross-meteor_score,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_24_general = 

#promisable features
attset_6_source = "lm_unk,l_tokens,berkeley-n,parse-dot,parse-comma,lt_errors,lt_DE_CASE,lt_BISSTRICH,lt_DE_AGREEMENT"
attset_6_target = "cross-meteor_score,lm_unk,berkeley-n,l_tokens,parse-dot,parse-comma,lt_errors,lt_EN_QUOTES,lt_EN_UNPAIRED_BRACKETS,lt_COMMA_PARENTHESIS_WHITESPACE,lt_DID_BASEFORM,lt_THREE_NN,lt_HE_VERB_AGR,lt_UPPERCASE_SENTENCE_START,lt_WORD_REPEAT_RULE"
attset_6_general =

#quest baseline (17). 1046 missing, 1074, 1075 supplemented by our parse-dot, parse-comma
attset_31_source = "parse-comma,parse-dot"
attset_31_target = "q_1001_1,q_1002_1,q_1006_1,q_1009_1,q_1012_1,q_1015_1,q_1022_1,q_1036_1,q_1049_1,q_1050_1,q_1053_1,q_1054_1,q_1057_1,q_1058_1,parse-comma,parse-dot"
attset_31_general =

#quest 50-set
attset_32_source = 
attset_32_target = "q_1057_1,q_1010_1,q_1005_1,q_1059_1,q_1002_1,q_1079_1,q_1003_1,q_1012_1,q_1078_1,q_1056_1,q_1004_1,q_1014_1,q_1013_1,q_1080_1,q_1024_1,q_1009_1,q_1030_1,q_1006_1,q_1034_1,q_1052_1,q_1062_1,q_1042_1,q_1061_1,q_1036_1,q_1032_1,q_1053_1,q_1044_1,q_1063_1,q_1050_1,q_1064_1,q_1051_1,q_1018_1,q_1082_1,q_1040_1,q_1022_1,q_1081_1,q_1077_1,q_1055_1,q_1060_1,q_1015_1,q_1054_1,q_1028_1,q_1038_1,q_1001_1,q_1058_1,q_1049_1,q_1016_1,q_1020_1,q_1011_1"
attset_32_general =

#quest baseline+ours
attset_33_source = "parse-comma,parse-dot,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_33_target = "q_1001_1,q_1002_1,q_1006_1,q_1009_1,q_1012_1,q_1015_1,q_1022_1,q_1036_1,q_1049_1,q_1050_1,q_1053_1,q_1054_1,q_1057_1,q_1058_1,parse-comma,parse-dot,cross-meteor_score,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_33_general = 

#de-en, wmtall, highest absolute relieff, n=15 
attset_41_source =
attset_41_target = "q_1077_1,cross-bleu,q_1062_1,q_1063_1,q_1078_1,q_1026_1,cross-meteor_fragPenalty,q_1081_1,parse-VVFIN,q_1079_1,q_1056_1,q_1078_1,lm_prob,q_1049_1"
attset_41_general =

#de-en, wmtall, highest absolute relieff, n=5 
attset_411_source =
attset_411_target = "q_1077_1,cross-bleu,q_1063_1,q_1062_1,q_1078_1"
attset_411_general =

#de-en, wmtall, highest positive relieff, a>0.01 
attset_412_source =
attset_412_target = "q_1062_1,q_1063_1,q_1026_1,q_1081_1,parse-VVFIN,lm_prob,q_1049_1"
attset_412_general =

#de-en, wmtall, highest absolute infogain, n=15, filtered by intuition
attset_42_source = "berkley-loglikelihood"
attset_42_target = "cross-meteor_fragPenalty,cross-meteor_recall,cross-meteor_precision,lm_prob,q_1009_1,q_1010_1,q_1016_1,q_1057_1,q_1057_1,q_1012_1,q_1022_1,berkley-loglikelihood,berkeley-n_ratio,q_1036_1"
attset_42_general =

#de-en, wmtall, highest absolute infogain, n=5, filtered by intuition
attset_422_source = "berkley-loglikelihood"
attset_422_target = "cross-meteor_score,lm_prob,q_1009_1,q_1010_1,q_1016_1"
attset_422_general =

#combination of 41 and 42
attset_43_source = "berkley-loglikelihood"
attset_43_target = "q_1062_1,q_1063_1,q_1026_1,q_1081_1,parse-VVFIN,lm_prob,q_1049_1,cross-meteor_fragPenalty,cross-meteor_recall,cross-meteor_precision,lm_prob,q_1009_1,q_1010_1,q_1016_1,q_1057_1,q_1057_1,q_1012_1,q_1022_1,berkley-loglikelihood,berkeley-n_ratio,q_1036_1"
attset_43_general =

#de-en, wmt09, highest infogain , n=7, a>0,027
attset_44_source = 
attset_44_target = "cross-meteor_fragPenalty,berkeley-n,q_1050_1,q_1060_1,cross-meteor_recall,cross-meteor_precision,berkeley-n"
attset_44_general =

#de-en, wmt09, highest infogain , n=9, a>0,02
attset_441_source = "berkeley-n" 
attset_441_target = "cross-meteor_fragPenalty,berkeley-n,q_1050_1,q_1060_1,cross-meteor_recall,cross-meteor_precision,berkeley-n,q_1006_1,lt_lt_UPPERCASE_SENTENCE_START_chars"
attset_441_general =

#combine 411 with 24
attset_413_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_413_target = "cross-meteor_score,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood,q_1077_1,cross-bleu,q_1063_1,q_1062_1,q_1078_1"
attset_413_general =

#combine 43 with 24
attset_431_source = "berkley-loglikelihood,lm_unk,l_tokens,berkeley-n,parse-VP"
attset_431_target = "q_1062_1,q_1063_1,q_1026_1,q_1081_1,parse-VVFIN,lm_prob,q_1049_1,cross-meteor_fragPenalty,cross-meteor_recall,cross-meteor_precision,lm_prob,q_1009_1,q_1010_1,q_1016_1,q_1057_1,q_1057_1,q_1012_1,q_1022_1,berkley-loglikelihood,berkeley-n_ratio,q_1036_1,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_431_general =

attset_800_source =
attset_800_target = "q1,q2,q5,q8,q11,q14,q18,q25,q30,q33,q34,q37,q38,q41,q42,q58,q59"
attset_800_general = 


meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff,tgt-1_ref-lev,tgt-1_ref-meteor_score,tgt-1_ref-meteor_fragPenalty,tgt-1_ref-meteor_recall,tgt-1_ref-meteor_precision,tgt-1_ref-bleu,tgt-2_ref-lev,tgt-2_ref-meteor_score,tgt-2_ref-meteor_fragPenalty,tgt-2_ref-meteor_recall,tgt-2_ref-meteor_precision,tgt-2_ref-bleu,tgt-1_rank,tgt-2_rank"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 


[eamt_old_baseline]
evaluation_invert_ranks = True
training_sets = "/share/taraxu/selection-mechanism/eamt09/data/text_split/eamt09.train.en-es.jcml"
test_set = "/share/taraxu/selection-mechanism/eamt09/data/text_split/eamt09.test.en-es.jcml"
att = ["attset_800"]

[eamt_previous]
evaluation_invert_ranks = True
attset_431_target = "q_1062_1,q_1063_1,q_1026_1,q_1081_1,lm_prob,q_1049_1,cross-meteor_fragPenalty,cross-meteor_recall,cross-meteor_precision,lm_prob,q_1009_1,q_1010_1,q_1016_1,q_1057_1,q_1057_1,q_1012_1,q_1022_1,berkley-loglikelihood,berkeley-n_ratio,q_1036_1,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
att = ["attset_24","attset_31","attset_413","attset_431"]

[eamt_old_baseline_invert]
invert_ranks = True
training_sets = "/share/taraxu/selection-mechanism/eamt09/data/text_split/eamt09.train.en-es.jcml"
test_set = "/share/taraxu/selection-mechanism/eamt09/data/text_split/eamt09.test.en-es.jcml"
att = ["attset_800"]

[eamt_previous_invert]
invert_ranks = True
attset_431_target = "q_1062_1,q_1063_1,q_1026_1,q_1081_1,lm_prob,q_1049_1,cross-meteor_fragPenalty,cross-meteor_recall,cross-meteor_precision,lm_prob,q_1009_1,q_1010_1,q_1016_1,q_1057_1,q_1057_1,q_1012_1,q_1022_1,berkley-loglikelihood,berkeley-n_ratio,q_1036_1,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
att = ["attset_24","attset_31","attset_413","attset_431"]


[eamt_previous_invert_oldvalues]

#quest baseline (17). 1046 missing, 1074, 1075 supplemented by our parse-dot, parse-comma
attset_831_source = "parse-comma,parse-dot"
attset_831_target = "q1,q2,q5,q8,q11,q14,q18,q25,q30,q33,q34,q37,q38,q41,q42,q58,q59,parse-comma,parse-dot"
attset_831_general =

#quest baseline+ours
attset_833_source = "parse-comma,parse-dot,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_833_target = "q1,q2,q5,q8,q11,q14,q18,q25,q30,q33,q34,q37,q38,q41,q42,q58,q59,parse-comma,parse-dot,cross-meteor_score,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_833_general = 

att = ["attset_831","attset_833"]

[eamt_previous_invert_oldvalues_invert]
invert_ranks = True
#quest baseline (17). 1046 missing, 1074, 1075 supplemented by our parse-dot, parse-comma
attset_831_source = "parse-comma,parse-dot"
attset_831_target = "q1,q2,q5,q8,q11,q14,q18,q25,q30,q33,q34,q37,q38,q41,q42,q58,q59,parse-comma,parse-dot"
attset_831_general =

#quest baseline+ours
path = /local/elav01/selection-mechanism/eamt09/autoranking
attset_833_source = "parse-comma,parse-dot,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_833_target = "q1,q2,q5,q8,q11,q14,q18,q25,q30,q33,q34,q37,q38,q41,q42,q58,q59,parse-comma,parse-dot,cross-meteor_score,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_833_general = 

training_sets = "/local/elav01/selection-mechanism/eamt09/autoranking/data/eamt09-train-en-es.all.analyzed.f.jcml"
test_set = "/local/elav01/selection-mechanism/eamt09/autoranking/data/eamt09-test-en-es.all.analyzed.f.jcml"


classifier = ["LibLinearLogReg", "LogReg", "kNN", "Naive"]
#att = ["attset_831","attset_833"]
att = ["attset_833"]



