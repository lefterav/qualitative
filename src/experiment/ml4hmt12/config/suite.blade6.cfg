[DEFAULT]
repetitions = 1
iterations = 130
path = /share/taraxu/selection-mechanism/autoranking

                  
                              
[ml4hmt]
experiment = grid
mode = "production"

langpair = ["es-en"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-combo-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-newstest-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/clean/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

class_name = ["ref-lev"]
binarize_ranks = True
invert_ranks = True
class_invert = True
#classifier = ["Naive", "kNN", "SVMEasy", "Tree", "LogReg", "C45", "CN2", "CN2Unordered", "CN2SDUnordered", "CN2EVCUnordered"]
classifier = ["Naive"]

params_LogReg = "{'stepwise_lr':True}"
#params_Tree = "{'minSubset':20; 'nodeLearner':LinearRegressionLearner()}" only for regression

attset_0_source = ""
attset_0_target = ""
attset_0_general = ""

att = ["attset_0","attset_2"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
 
 
[autoranking_featuresets_2]

experiment = grid
mode = "production"

langpair = ["de-en","en-de"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-combo-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-newstest-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/clean/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

class_name = ["rank"]
class_type = "d"

classifier = ["Naive", "kNN", "SVMEasy", "Tree", "LogReg", "C45", "CN2", "CN2Unordered", "CN2SDUnordered", "CN2EVCUnordered"]

params_LogReg = "{'stepwise_lr':True}"
#params_Tree = "{'minSubset':20; 'nodeLearner':LinearRegressionLearner()}" only for regression

attset_1_source = ""
attset_1_target = ""
attset_1_general = ""

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

#succesful features from our WMT11 metric with no ratios
attset_21_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_target = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_general = ""

#SVR of Soricut et. al 2012 (Best WMT12) [subset]
attset_3_source = "l_tokens,l_avgchars,lm_prob,lm_unk,parse-dot,parse-comma"
attset_3_target = "bleu_cross,lm_prob"
attset_3_general = ""

#M5P of Soricut et. al 2012 (Best WMT12) [subset]
attset_4_source = "l_tokens,l_avgchars,lm_prob,parse-dot,parse-comma"
attset_4_target = "bleu_cross,lm_prob"
attset_4_general = ""

#Felice&Specia 2012 (Baseline WMT12) [subset]
attset_5_source = "l_avgchars,lm_prob,parse-NP,l_tokens,"
attset_5_target = "berkley-loglikelihood,lm_unk,parse-VP_ratio,parse-PP_ratio,parse-NP,parse-V,l_tokens_ratio"
attset_5_general = ""

att = ["attset_2","attset_21","attset_3","attset_4","attset_5","attset_1"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
 




[autoranking_featuresets_2_dev]

experiment = grid
mode = "production"

langpair = ["de-en","en-de"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/data/jcml/jcml-latest/dev/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/dev/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

class_name = ["rank"]
class_type = "d"

classifier = ["Naive", "kNN", "SVMEasy", "Tree", "LogReg", "C45", "CN2", "CN2Unordered", "CN2SDUnordered", "CN2EVCUnordered"]

params_LogReg = "{'stepwise_lr':True}"
#params_Tree = "{'minSubset':20; 'nodeLearner':LinearRegressionLearner()}" only for regression

attset_1_source = ""
attset_1_target = ""
attset_1_general = ""

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

#succesful features from our WMT11 metric with no ratios
attset_21_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_target = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_general = ""

#SVR of Soricut et. al 2012 (Best WMT12) [subset]
attset_3_source = "l_tokens,l_avgchars,lm_prob,lm_unk,parse-dot,parse-comma"
attset_3_target = "bleu_cross,lm_prob"
attset_3_general = ""

#M5P of Soricut et. al 2012 (Best WMT12) [subset]
attset_4_source = "l_tokens,l_avgchars,lm_prob,parse-dot,parse-comma"
attset_4_target = "bleu_cross,lm_prob"
attset_4_general = ""

#Felice&Specia 2012 (Baseline WMT12) [subset]
attset_5_source = "l_avgchars,lm_tri-prob,parse-NP,l_tokens,"
attset_5_target = "berkley-loglikelihood,lm_unk,parse-VP_ratio,parse-PP_ratio,parse-NP,parse-V,l_tokens_ratio"
attset_5_general = ""

att = ["attset_2","attset_21","attset_3","attset_4","attset_5","attset_1"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
 


[autoranking_featuresets_2_dev_1]

experiment = grid
mode = "production"

langpair = ["de-en"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/data/jcml/jcml-latest/dev/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/dev/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

class_name = ["rank"]
class_type = "d"

classifier = ["SVMEasy"]
#, "C45"]

params_LogReg = "{'stepwise_lr':True}"
#params_Tree = "{'minSubset':20; 'nodeLearner':LinearRegressionLearner()}" only for regression

attset_1_source = ""
attset_1_target = ""
attset_1_general = ""

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

#succesful features from our WMT11 metric with no ratios
attset_21_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_target = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_general = ""

#SVR of Soricut et. al 2012 (Best WMT12) [subset]
attset_3_source = "l_tokens,l_avgchars,lm_prob,lm_unk,parse-dot,parse-comma"
attset_3_target = "bleu_cross,lm_prob"
attset_3_general = ""

#M5P of Soricut et. al 2012 (Best WMT12) [subset]
attset_4_source = "l_tokens,l_avgchars,lm_prob,parse-dot,parse-comma"
attset_4_target = "bleu_cross,lm_prob"
attset_4_general = ""

#Felice&Specia 2012 (Baseline WMT12) [subset]
attset_5_source = "l_avgchars,lm_tri-prob,parse-NP,l_tokens,"
attset_5_target = "berkley-loglikelihood,lm_unk,parse-VP_ratio,parse-PP_ratio,parse-NP,parse-V,l_tokens_ratio"
attset_5_general = ""

att = ["attset_2"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
 
 
[autoranking_wmt11]

experiment = grid
mode = "production"

langpair = ["de-en", "en-de"]

ties = False
include_references = False

#raw, clean
trainset_mode = "annotated"

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml"


class_name = "rank"
class_type = "d"

classifier = "Naive"

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

att = "attset_2"

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
 
[autoranking_featuresets_2b]

experiment = grid
mode = "production"

langpair = ["de-en","en-de"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-combo-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-newstest-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/clean/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

class_name = ["rank"]
class_type = "d"

classifier = ["CN2", "CN2Unordered", "CN2SDUnordered", "CN2EVCUnordered"]

params_LogReg = "{'stepwise_lr':True}"
#params_Tree = "{'minSubset':20; 'nodeLearner':LinearRegressionLearner()}" only for regression

attset_1_source = ""
attset_1_target = ""
attset_1_general = ""

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

#succesful features from our WMT11 metric with no ratios
attset_21_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_target = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_general = ""

#SVR of Soricut et. al 2012 (Best WMT12) [subset]
attset_3_source = "l_tokens,l_avgchars,lm_prob,lm_unk,parse-dot,parse-comma"
attset_3_target = "bleu_cross,lm_prob"
attset_3_general = ""

#M5P of Soricut et. al 2012 (Best WMT12) [subset]
attset_4_source = "l_tokens,l_avgchars,lm_prob,parse-dot,parse-comma"
attset_4_target = "bleu_cross,lm_prob"
attset_4_general = ""

#Felice&Specia 2012 (Baseline WMT12) [subset]
attset_5_source = "l_avgchars,lm_tri-prob,parse-NP,l_tokens,"
attset_5_target = "berkley-loglikelihood,lm_unk,parse-VP_ratio,parse-PP_ratio,parse-NP,parse-V,l_tokens_ratio"
attset_5_general = ""

att = ["attset_2","attset_21","attset_3","attset_4","attset_5","attset_1"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
 

[autoranking_wmt11_cleantest]

experiment = grid
mode = "production"

langpair = ["de-en", "en-de"]

ties = False
include_references = False

#raw, clean
trainset_mode = "annotated"

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/clean/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml"


class_name = "rank"
class_type = "d"

classifier = "Naive"

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

att = "attset_2"

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 


[autoranking_wmt11_test11_cleantest]


experiment = grid
mode = "production"

langpair = ["de-en", "en-de"]

ties = False
include_references = False

#raw, clean
trainset_mode = "annotated"

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/clean/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml"


class_name = "rank"
class_type = "d"

classifier = "Naive"

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

att = "attset_2"

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 


[autoranking_featuresets_2_uncleantest_naive]

experiment = grid
mode = "production"

langpair = ["de-en"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-combo-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-newstest-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

class_name = ["rank"]
class_type = "d"

classifier = ["Naive","kNN"]
#, "kNN", "SVMEasy", "Tree", "LogReg", "C45", "CN2", "CN2Unordered", "CN2SDUnordered", "CN2EVCUnordered"

params_LogReg = "{'stepwise_lr':True}"
#params_Tree = "{'minSubset':20; 'nodeLearner':LinearRegressionLearner()}" only for regression

attset_1_source = ""
attset_1_target = ""
attset_1_general = ""

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

#succesful features from our WMT11 metric with no ratios
attset_21_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_target = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_general = ""

#SVR of Soricut et. al 2012 (Best WMT12) [subset]
attset_3_source = "l_tokens,l_avgchars,lm_prob,lm_unk,parse-dot,parse-comma"
attset_3_target = "bleu_cross,lm_prob"
attset_3_general = ""

#M5P of Soricut et. al 2012 (Best WMT12) [subset]
attset_4_source = "l_tokens,l_avgchars,lm_prob,parse-dot,parse-comma"
attset_4_target = "bleu_cross,lm_prob"
attset_4_general = ""

#Felice&Specia 2012 (Baseline WMT12) [subset]
attset_5_source = "l_avgchars,lm_tri-prob,parse-NP,l_tokens,"
attset_5_target = "berkley-loglikelihood,lm_unk,parse-VP_ratio,parse-PP_ratio,parse-NP,parse-V,l_tokens_ratio"
attset_5_general = ""

att = ["attset_2"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
 

[autoranking_featuresets_2_uncleantest_logreg]

experiment = grid
mode = "production"

langpair = ["de-en"]

ties = [False]
include_references = [False]

#raw, clean
trainset_mode = ["annotated"]

training_sets = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2008-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-combo-{langpair}-jcml-rank.all.analyzed.f.jcml,/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2011-newstest-{langpair}-jcml-rank.all.analyzed.f.jcml"
test_set = "/share/taraxu/data/jcml/jcml-latest/{trainset_mode}/wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

class_name = ["rank"]
class_type = "d"

classifier = ["LogReg"]
#, "kNN", "SVMEasy", "Tree", "LogReg", "C45", "CN2", "CN2Unordered", "CN2SDUnordered", "CN2EVCUnordered"

params_LogReg = "{'stepwise_lr':True}"
#params_Tree = "{'minSubset':20; 'nodeLearner':LinearRegressionLearner()}" only for regression

attset_1_source = ""
attset_1_target = ""
attset_1_general = ""

#succesful features from our WMT11 metric
attset_2_source = "lm_unk"
attset_2_target = "lm_unk,l_tokens_ratio,berkeley-n_ratio,parse-VP_ratio,berkley-loglikelihood_ratio"
attset_2_general = ""

#succesful features from our WMT11 metric with no ratios
attset_21_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_target = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_21_general = ""

#SVR of Soricut et. al 2012 (Best WMT12) [subset]
attset_3_source = "l_tokens,l_avgchars,lm_prob,lm_unk,parse-dot,parse-comma"
attset_3_target = "bleu_cross,lm_prob"
attset_3_general = ""

#M5P of Soricut et. al 2012 (Best WMT12) [subset]
attset_4_source = "l_tokens,l_avgchars,lm_prob,parse-dot,parse-comma"
attset_4_target = "bleu_cross,lm_prob"
attset_4_general = ""

#Felice&Specia 2012 (Baseline WMT12) [subset]
attset_5_source = "l_avgchars,lm_tri-prob,parse-NP,l_tokens,"
attset_5_target = "berkley-loglikelihood,lm_unk,parse-VP_ratio,parse-PP_ratio,parse-NP,parse-V,l_tokens_ratio"
attset_5_general = ""

att = ["attset_21"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 


[wmt11_old_data]
#compare performance on wmt11 data setting

experiment = grid
mode = "production"

langpair = ["de-en"]

ties = [False]
include_references = [False]

trainset_mode = "annotated"

training_sets = "/home/elav01/tools/TaraXUscripts/data/multiclass/wmt08.if.jcml,/home/elav01/tools/TaraXUscripts/data/multiclass/wmt10-train.partial.if.jcml"
test_set = "/home/elav01/tools/TaraXUscripts/data/multiclass/wmt10-test.if.jcml"
#,"/share/taraxu/data/jcml/wmt11combotest.autoranking.sax.final.out.jcml","/share/taraxu/data/jcml/wmt11newstest.out.fixed.jcml"]

class_name = "rank"
class_type = "d"

classifier = "Naive"

attset_2011_source = 
attset_2011_target = "unk,length_ratio,berkeley-n_ratio,parse-VP_ratio,parse-NP_ratio,berkeley-avg-confidence_ratio"
attset_2011_general = 

att = ["attset_2011"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 


[wmt11_old_data_combo]
#see if predicted stuff match wmt11combo

experiment = grid
mode = "production"

langpair = ["de-en"]

ties = [False]
include_references = [False]

trainset_mode = "annotated"

training_sets = "/home/elav01/tools/TaraXUscripts/data/multiclass/wmt08.if.jcml,/home/elav01/tools/TaraXUscripts/data/multiclass/wmt10-train.partial.if.jcml"
test_set = "/share/taraxu/data/jcml/wmt11combotest.autoranking.sax.final.out.jcml"

class_name = "rank"
class_type = "d"

classifier = "Naive"

attset_2011_source = 
attset_2011_target = "unk,length_ratio,berkeley-n_ratio,parse-VP_ratio,parse-NP_ratio,berkeley-avg-confidence_ratio"
attset_2011_general = 

att = ["attset_2011"]

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 
