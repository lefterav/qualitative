[DEFAULT]

repetitions = 1
iterations = 4
experiment = "grid"

path = /home/elav01/taraxu_data/qualitative/

class_name = "rank"
langpair = "de-en"

meta_attributes = "testset,judgement_id,langsrc,langtgt,ps1_judgement_id,ps2_judgement_id,id,tgt-1_score,tgt-1_system,tgt-2_score,tgt-2_system,document_id,judge_id,segment_id"
hidden_attributes = "tgt-1_berkeley-tree,tgt-2_berkeley-tree,src_berkeley-tree,rank_diff,tgt-1_ref-lev,tgt-1_ref-meteor_score,tgt-1_ref-meteor_fragPenalty,tgt-1_ref-meteor_recall,tgt-1_ref-meteor_precision,tgt-1_ref-bleu,tgt-2_ref-lev,tgt-2_ref-meteor_score,tgt-2_ref-meteor_fragPenalty,tgt-2_ref-meteor_recall,tgt-2_ref-meteor_precision,tgt-2_ref-bleu,tgt-1_rank,tgt-2_rank"
discrete_attributes = "src_reuse_status,src_terminologyAdmitted_status,src_total_status,src_spelling_status,src_style_status,src_grammar_status,src_terminology_status,src_resultStats_projectStatus,tgt-1_reuse_status,tgt-1_terminologyAdmitted_status,tgt-1_total_status,tgt-1_spelling_status,tgt-1_style_status,tgt-1_grammar_status,tgt-1_terminology_status,tgt-1_resultStats_projectStatus,tgt-2_reuse_status,tgt-2_terminologyAdmitted_status,tgt-2_total_status,tgt-2_spelling_status,tgt-2_style_status,tgt-2_grammar_status,tgt-2_terminology_status,tgt-2_resultStats_projectStatus" 

training_path = "/local/tmp/elav01/selection-mechanism/autoranking/annotation/annotated_de-en/9"
training_sets = "wmt2012-newstest-{langpair}-rank.all.analyzed.f.jcml,wmt2010-{langpair}-jcml-rank.all.analyzed.f.jcml,wmt2011-combo-{langpair}-jcml-rank.all.analyzed.f.jcml,wmt2011-newstest-{langpair}-jcml-rank.all.analyzed.f.jcml,wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

test = "list"
#test = "crossvalidation"
test_path = "/local/tmp/elav01/selection-mechanism/autoranking/annotation/annotated_de-en/9"
test_set = "wmt2009-{langpair}-jcml-rank.all.analyzed.f.jcml"

tempdir = "/local/tmp/elav01/tmp"
#tempdir = "/home/dupo/taraxu_data/qualitative/tmp"

params_LogRegLearner = "{'stepwise_lr':True}"
params_SVMEasyLearner = "{'verbose':True}"


[dev]
#succesful features from our WMT11 metric with no ratios + pseudoMETEOR
attset_00_source = "l_tokens"
attset_00_target = "cross-meteor_score,l_tokens"
attset_00_general = None
remove_infinite = False
att = ["attset_00"]
learner = ["LogRegLearner"]
#"LogRegLearner", "kNNLearner", "TreeLearner", "C45Learner", "SVMEasyLearner"]

[coling2012]
#succesful features from our WMT11 metric with no ratios + pseudoMETEOR
attset_24_source = "lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_24_target = "cross-meteor_score,lm_unk,l_tokens,berkeley-n,parse-VP,berkley-loglikelihood"
attset_24_general = None
att = ["attset_24"]
learner = ["LogRegLearner"]
bidirectional_pairs = False